# **Core Concepts**

## 1. What is S3?
Amazon S3 (Simple Storage Service) is a scalable, secure, and durable object storage service offered by AWS. It’s designed to store and retrieve any amount of data from anywhere on the web

## 2. **Buckets and Objects**
In **Amazon S3**, the two fundamental components are **buckets** and **objects**. Here's a clear explanation of each:
#### **Buckets**
A **bucket** is a container for storing objects in S3.
##### Key Characteristics:
- You create a bucket in a specific **AWS region**.
- Bucket names must be **globally unique**.
- You can configure:
  - **Permissions** (via IAM, bucket policies, ACLs)
  - **Versioning**
  - **Lifecycle rules**
  - **Logging**
  - **Encryption**
#### **Objects**
An **object** is the actual data stored in a bucket.
##### Each object consists of:
1. **Data** (e.g., file content)
2. **Key** (unique identifier within the bucket)
3. **Metadata** (e.g., content type, custom tags)
4. **Version ID** (if versioning is enabled)
#### Relationship
- A **bucket** can contain **many objects**.
- Each object is uniquely identified by its **key** within the bucket.

## 3. **S3 Storage Classes**

| **Storage Class**             | **Use Case**                                      | **Retrieval Time** | **Recommended Usage Period** | **Cost**     |
|------------------------------|---------------------------------------------------|--------------------|------------------------------|--------------|
| **S3 Standard**              | Frequently accessed data                          | Milliseconds       | Minutes to daily access      |  High      |
| **S3 Intelligent-Tiering**  | Unpredictable access patterns                     | Milliseconds       | Minutes to monthly access    |  Adaptive  |
| **S3 Standard-IA**          | Infrequently accessed, but needed quickly         | Milliseconds       | Days to monthly access       |  Lower     |
| **S3 One Zone-IA**          | Non-critical infrequent access (single AZ)        | Milliseconds       | Days to monthly access       |  Lower     |
| **S3 Glacier Instant**      | Archived data with occasional instant access      | Milliseconds       | Monthly to quarterly access  |  Low       |
| **S3 Glacier Flexible**     | Long-term archives with flexible retrieval        | Minutes to hours   | Quarterly to yearly access   |  Very Low  |
| **S3 Glacier Deep Archive**| Compliance or rarely accessed long-term backups   | Hours              | Yearly or longer             |  Lowest    |

#### **Quick Decision Guide**
- **Need fast access?** → Standard or Intelligent-Tiering
- **Access once a month?** → Standard-IA or One Zone-IA
- **Archive but need instant access?** → Glacier Instant
- **Archive with flexible access?** → Glacier Flexible
- **Rarely accessed, long-term storage?** → Glacier Deep Archive

## 4. **Data Consistency Model**

| **Operation**        | **Consistency Behavior**                                                                 |
|----------------------|-------------------------------------------------------------------------------------------|
| **PUT (Create/Update)** | Immediately visible after success. You can read the object right after writing it.       |
| **DELETE**           | Immediately removed. A subsequent GET will not return the object.                        |
| **LIST**             | Immediately reflects changes (new objects appear, deleted ones disappear).               |
| **GET**              | Always returns the latest version of the object.                                         |
| **HEAD**             | Consistent with GET; reflects latest metadata.                                           |
| **COPY**             | Strongly consistent; copied object is immediately available.                             |
| **MULTIPART UPLOAD** | Finalized object is immediately available after completion.                              |

#### **What Strong Consistency Means**
- You don’t need to implement retry logic or delays to ensure visibility of changes.
- It simplifies application logic, especially for distributed systems.

# **Security and Access Control**

## 5. **Bucket Policies**
## 6. **IAM Policies for S3**
## 7. **Access Control Lists (ACLs)**
## 8. **S3 Block Public Access**
## 9. **Encryption**
   - SSE-S3, SSE-KMS, SSE-C
## 10. **VPC Endpoints for S3**

# **Storage Management**

## 11. **Lifecycle Policies**
## 12. **Object Locking and Retention**
## 13. **Versioning**
## 14. **Replication**
   - Cross-Region Replication (CRR)
   - Same-Region Replication (SRR)

# **Performance and Optimization**

## 15. **Multipart Uploads**
**S3 Multipart Upload** is a feature that allows you to upload a **single large object as a set of parts**. Each part is uploaded independently and in parallel, and then S3 assembles them into a single object.

### **When to Use Multipart Upload**

Use it when:
| **Condition** | **Why** |
|---------------|---------|
| Object size > 100 MB (recommended) | Improves upload performance and reliability |
| Object size > 5 GB (required) | Multipart upload is **mandatory** for files > 5 GB |
| You want to **resume uploads** after failure | You can retry failed parts without re-uploading the whole file |
| You want to **upload in parallel** | Speeds up large uploads using multiple threads or processes |

### **When Not to Use Multipart Upload**
Avoid it when:
| **Condition** | **Why** |
|---------------|---------|
| Files are small (< 100 MB) | Overhead of managing parts outweighs benefits |
| You don’t need parallelism or resumability | Simpler to use a single PUT operation |
| You’re uploading from a constrained environment | Multipart logic adds complexity to client code |

### **How It Works (Steps)**

1. **Initiate** the multipart upload
2. **Upload parts** (each part must be 5 MB–5 GB, except the last one)
3. **Complete** the upload (S3 assembles the parts)
4. Optionally, **abort** the upload if needed
Here’s a list of the **available AWS S3 Multipart Upload APIs** and their purposes:

### **S3 Multipart Upload APIs**

| **API**                         | **Purpose**                                                                 |
|----------------------------------|-----------------------------------------------------------------------------|
| `create_multipart_upload`        | Initiates a multipart upload and returns an `UploadId`.                    |
| `upload_part`                    | Uploads a single part of the file. Requires `PartNumber` and `UploadId`.  |
| `upload_part_copy`               | Uploads a part by copying data from an existing object.                    |
| `list_parts`                     | Lists the parts that have been uploaded for a specific `UploadId`.         |
| `complete_multipart_upload`      | Finalizes the upload by assembling all parts into a single object.         |
| `abort_multipart_upload`         | Cancels the upload and deletes all uploaded parts.                         |
| `list_multipart_uploads`         | Lists all ongoing multipart uploads in a bucket.                           |

### **Typical Workflow**

1. **Start**: `create_multipart_upload`
2. **Upload**: `upload_part` (repeat for each part)
3. **Monitor**: `list_parts` or `list_multipart_uploads`
4. **Finish**: `complete_multipart_upload`
5. **Cancel (if needed)**: `abort_multipart_upload`
Here’s a **high-level process** for handling **failures in S3 Multipart Uploads** and how to **safely retry**:

### **Multipart Upload with Failure Handling & Retry**

#### **1. Initiate Upload**
- Start with `create_multipart_upload`
- Save the `UploadId` securely (e.g., in a file or database)

#### **2. Upload Parts with Retry Logic**
- Split the file into 5 MB chunks
- For each part:
  - Try uploading using `upload_part`
  - If it fails:
    - Retry up to 3 times with exponential backoff
    - Log errors and continue

#### **3. Handle Crashes or Interruptions**
If the program crashes:
- Use the saved `UploadId`
- Call `list_parts` to see which parts were successfully uploaded
- Resume uploading only the missing parts

#### **4. Complete Upload**
Once all parts are uploaded:
- Call `complete_multipart_upload` with the list of `ETag`s and `PartNumber`s

#### **5. Abort if Needed**
If the upload cannot be completed:
- Call `abort_multipart_upload` to clean up and avoid storage charges

### **Best Practices**
- Always **persist `UploadId` and part metadata**
- Use **`list_multipart_uploads`** to monitor active uploads
- Implement **logging and alerting** for failures
- Use **parallel uploads** for performance, but handle retries independently
```python
import os
import boto3
import logging
import time

# Configure logging
logging.basicConfig(level=logging.INFO)

# AWS credentials from environment variables
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
aws_session_token = os.getenv('AWS_SESSION_TOKEN')  # Optional

# Validate credentials
if not aws_access_key_id or not aws_secret_access_key:
    raise EnvironmentError("AWS credentials not found in environment variables.")

# Create S3 client using environment credentials
s3 = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    aws_session_token=aws_session_token
)

# Configuration
bucket_name = 'your-bucket-name'
file_path = 'your-large-file.zip'
key_name = 'uploads/your-large-file.zip'
chunk_size = 5 * 1024 * 1024  # 5 MB
max_retries = 3

# Step 1: Initiate multipart upload
try:
    response = s3.create_multipart_upload(Bucket=bucket_name, Key=key_name)
    upload_id = response['UploadId']
    logging.info(f"Initiated multipart upload with ID: {upload_id}")
except Exception as e:
    logging.error(f"Failed to initiate multipart upload: {e}")
    raise

# Step 2: Upload parts with retry logic
parts = []
part_number = 1

try:
    with open(file_path, 'rb') as f:
        while True:
            data = f.read(chunk_size)
            if not data:
                break

            for attempt in range(max_retries):
                try:
                    part = s3.upload_part(
                        Bucket=bucket_name,
                        Key=key_name,
                        PartNumber=part_number,
                        UploadId=upload_id,
                        Body=data
                    )
                    parts.append({'ETag': part['ETag'], 'PartNumber': part_number})
                    logging.info(f"Uploaded part {part_number}")
                    break
                except Exception as e:
                    logging.warning(f"Error uploading part {part_number}, attempt {attempt + 1}: {e}")
                    time.sleep(2 ** attempt)
            else:
                raise Exception(f"Failed to upload part {part_number} after {max_retries} attempts.")

            part_number += 1

    # Step 3: Complete multipart upload
    s3.complete_multipart_upload(
        Bucket=bucket_name,
        Key=key_name,
        UploadId=upload_id,
        MultipartUpload={'Parts': parts}
    )
    logging.info("Multipart upload completed successfully.")

except Exception as e:
    logging.error(f"Multipart upload failed: {e}")
    # Abort the upload to avoid orphaned parts
    try:
        s3.abort_multipart_upload(Bucket=bucket_name, Key=key_name, UploadId=upload_id)
        logging.info("Multipart upload aborted.")
    except Exception as abort_error:
        logging.error(f"Failed to abort multipart upload: {abort_error}")
```
## 16. **Transfer Acceleration**
## 17. **Event Notifications**
S3 Event Notifications let you hook into bucket-level object events (create, delete, restore, replication) and push them to SNS topics, SQS queues, or Lambda functions. They’re perfect for real-time triggers in decoupled, serverless pipelines but aren’t the best choice when you need strict ordering, guaranteed delivery at massive scale, or full audit trails—where services like EventBridge or CloudTrail excel instead. You can configure notifications interactively in the AWS Console or programmatically using boto3’s `put_bucket_notification_configuration` API.

#### What Are S3 Event Notifications?

S3 Event Notifications emit JSON event records whenever specific object-level actions occur.  

- Supported actions: `ObjectCreated:*`, `ObjectRemoved:*`, `ObjectRestore:*`, replication events, lifecycle transitions.  
- Destinations:  
  - **Lambda** for immediate, serverless processing  
  - **SQS** for reliable, decoupled queueing and batching  
  - **SNS** for pub/sub fan-out to multiple subscribers

#### When to Use Them

Use S3 Event Notifications for:

- Real-time processing of newly uploaded files (e.g., image thumbnailing)  
- Asynchronous ETL pipelines that kick off on object arrival  
- Decoupled microservices: object storage → event → worker  
- Simple pub/sub workflows with fan-out (SNS → multiple Lambdas/SQS)  
- Lightweight, on-the-fly data validation or virus scanning  

#### When Not to Use Them

Avoid S3 Event Notifications if you need:

- Strict ordering or exactly-once delivery under high throughput (>5,000 events/sec)  
- Full, immutable audit trails of every bucket operation (use CloudTrail Lake)  
- Cross-region event routing (EventBridge has built-in cross-region support)  
- Complex event patterns or transformations before delivery (EventBridge rule engine)  
- Guaranteed delivery retries beyond default Lambda/SQS DLQ limits  

#### Setting Up via AWS Management Console

1. Navigate to the S3 console and select your bucket.  
2. Go to the **Properties** tab, scroll to **Event notifications**, then click **Create event notification**.  
3. Enter a name for the notification.  
4. Under **Event types**, choose one or more (e.g., **All object create events**).  
5. (Optional) Add **Prefix** and/or **Suffix** filters to narrow which keys trigger events (e.g., `images/`, `.jpg`).  
6. Under **Destination**, pick **Lambda function**, **SQS queue**, or **SNS topic**.  
7. Select or enter the ARN of the destination resource.  
8. Click **Save changes**.  

#### Setting Up via boto3

```python
import boto3

# Initialize the client
s3 = boto3.client('s3')

# Define notification configuration
notification_config = {
    'LambdaFunctionConfigurations': [
        {
            'Id': 'ImageUploadTrigger',
            'LambdaFunctionArn': 'arn:aws:lambda:us-east-1:123456789012:function:ProcessImage',
            'Events': ['s3:ObjectCreated:Put'],
            'Filter': {
                'Key': {
                    'FilterRules': [
                        {'Name': 'prefix', 'Value': 'uploads/'},
                        {'Name': 'suffix', 'Value': '.jpg'}
                    ]
                }
            }
        }
    ]
}

# Apply configuration to your bucket
s3.put_bucket_notification_configuration(
    Bucket='my-photo-bucket',
    NotificationConfiguration=notification_config
)

print("Notification configured successfully.")
```

#### Best Practices & Pitfalls

- Design consumers to be **idempotent**; S3 may deliver duplicates.  
- Attach a **dead-letter queue** (DLQ) for Lambda failures or undeliverable SQS messages.  
- Monitor invocation metrics in CloudWatch (Errors, Throttles).  
- Limit filter rules—buckets have a 1,000-rule maximum across all configurations.  
- Ensure proper IAM permissions:  
  - `s3:PutBucketNotification` on the bucket  
  - `lambda:InvokeFunction` if you target Lambda  

## 18. **Requester Pays Buckets**

# **Integration and Tools**

## 19. **S3 with AWS Lambda**
## 20. **S3 with AWS Glue and Athena**
## 21. **S3 with CloudFront**
## 22. **S3 with Data Lake Architecture**

# **Monitoring and Logging**

## 23. **S3 Access Logs**
## 24. **CloudTrail Integration**
## 25. **CloudWatch Metrics for S3**

# **Advanced Features**

## 26. **S3 Select**
## 27. **S3 Object Lambda**
## 28. **S3 Inventory**
## 29. **S3 Batch Operations**
