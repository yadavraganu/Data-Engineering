# **Core Concepts**

## 1. What is S3?
Amazon S3 (Simple Storage Service) is a scalable, secure, and durable object storage service offered by AWS. It’s designed to store and retrieve any amount of data from anywhere on the web

## 2. **Buckets and Objects**
In **Amazon S3**, the two fundamental components are **buckets** and **objects**. Here's a clear explanation of each:
#### **Buckets**
A **bucket** is a container for storing objects in S3.
##### Key Characteristics:
- You create a bucket in a specific **AWS region**.
- Bucket names must be **globally unique**.
- You can configure:
  - **Permissions** (via IAM, bucket policies, ACLs)
  - **Versioning**
  - **Lifecycle rules**
  - **Logging**
  - **Encryption**
#### **Objects**
An **object** is the actual data stored in a bucket.
##### Each object consists of:
1. **Data** (e.g., file content)
2. **Key** (unique identifier within the bucket)
3. **Metadata** (e.g., content type, custom tags)
4. **Version ID** (if versioning is enabled)
#### Relationship
- A **bucket** can contain **many objects**.
- Each object is uniquely identified by its **key** within the bucket.

## 3. **S3 Storage Classes**

| **Storage Class**             | **Use Case**                                      | **Retrieval Time** | **Recommended Usage Period** | **Cost**     |
|------------------------------|---------------------------------------------------|--------------------|------------------------------|--------------|
| **S3 Standard**              | Frequently accessed data                          | Milliseconds       | Minutes to daily access      |  High      |
| **S3 Intelligent-Tiering**  | Unpredictable access patterns                     | Milliseconds       | Minutes to monthly access    |  Adaptive  |
| **S3 Standard-IA**          | Infrequently accessed, but needed quickly         | Milliseconds       | Days to monthly access       |  Lower     |
| **S3 One Zone-IA**          | Non-critical infrequent access (single AZ)        | Milliseconds       | Days to monthly access       |  Lower     |
| **S3 Glacier Instant**      | Archived data with occasional instant access      | Milliseconds       | Monthly to quarterly access  |  Low       |
| **S3 Glacier Flexible**     | Long-term archives with flexible retrieval        | Minutes to hours   | Quarterly to yearly access   |  Very Low  |
| **S3 Glacier Deep Archive**| Compliance or rarely accessed long-term backups   | Hours              | Yearly or longer             |  Lowest    |

#### **Quick Decision Guide**
- **Need fast access?** → Standard or Intelligent-Tiering
- **Access once a month?** → Standard-IA or One Zone-IA
- **Archive but need instant access?** → Glacier Instant
- **Archive with flexible access?** → Glacier Flexible
- **Rarely accessed, long-term storage?** → Glacier Deep Archive

## 4. **Data Consistency Model**

| **Operation**        | **Consistency Behavior**                                                                 |
|----------------------|-------------------------------------------------------------------------------------------|
| **PUT (Create/Update)** | Immediately visible after success. You can read the object right after writing it.       |
| **DELETE**           | Immediately removed. A subsequent GET will not return the object.                        |
| **LIST**             | Immediately reflects changes (new objects appear, deleted ones disappear).               |
| **GET**              | Always returns the latest version of the object.                                         |
| **HEAD**             | Consistent with GET; reflects latest metadata.                                           |
| **COPY**             | Strongly consistent; copied object is immediately available.                             |
| **MULTIPART UPLOAD** | Finalized object is immediately available after completion.                              |

#### **What Strong Consistency Means**
- You don’t need to implement retry logic or delays to ensure visibility of changes.
- It simplifies application logic, especially for distributed systems.

# **Security and Access Control**

## 5. **Bucket Policies**
## 6. **IAM Policies for S3**
## 7. **Access Control Lists (ACLs)**
## 8. **S3 Block Public Access**
## 9. **Encryption**
   - SSE-S3, SSE-KMS, SSE-C
## 10. **VPC Endpoints for S3**

# **Storage Management**

## 11. **Lifecycle Policies**
## 12. **Object Locking and Retention**
## 13. **Versioning**
## 14. **Replication**
   - Cross-Region Replication (CRR)
   - Same-Region Replication (SRR)

# **Performance and Optimization**

## 15. **Multipart Uploads**
**S3 Multipart Upload** is a feature that allows you to upload a **single large object as a set of parts**. Each part is uploaded independently and in parallel, and then S3 assembles them into a single object.

### **When to Use Multipart Upload**

Use it when:
| **Condition** | **Why** |
|---------------|---------|
| Object size > 100 MB (recommended) | Improves upload performance and reliability |
| Object size > 5 GB (required) | Multipart upload is **mandatory** for files > 5 GB |
| You want to **resume uploads** after failure | You can retry failed parts without re-uploading the whole file |
| You want to **upload in parallel** | Speeds up large uploads using multiple threads or processes |

### **When Not to Use Multipart Upload**
Avoid it when:
| **Condition** | **Why** |
|---------------|---------|
| Files are small (< 100 MB) | Overhead of managing parts outweighs benefits |
| You don’t need parallelism or resumability | Simpler to use a single PUT operation |
| You’re uploading from a constrained environment | Multipart logic adds complexity to client code |

### **How It Works (Steps)**

1. **Initiate** the multipart upload
2. **Upload parts** (each part must be 5 MB–5 GB, except the last one)
3. **Complete** the upload (S3 assembles the parts)
4. Optionally, **abort** the upload if needed
Here’s a list of the **available AWS S3 Multipart Upload APIs** and their purposes:

### **S3 Multipart Upload APIs**

| **API**                         | **Purpose**                                                                 |
|----------------------------------|-----------------------------------------------------------------------------|
| `create_multipart_upload`        | Initiates a multipart upload and returns an `UploadId`.                    |
| `upload_part`                    | Uploads a single part of the file. Requires `PartNumber` and `UploadId`.  |
| `upload_part_copy`               | Uploads a part by copying data from an existing object.                    |
| `list_parts`                     | Lists the parts that have been uploaded for a specific `UploadId`.         |
| `complete_multipart_upload`      | Finalizes the upload by assembling all parts into a single object.         |
| `abort_multipart_upload`         | Cancels the upload and deletes all uploaded parts.                         |
| `list_multipart_uploads`         | Lists all ongoing multipart uploads in a bucket.                           |

### **Typical Workflow**

1. **Start**: `create_multipart_upload`
2. **Upload**: `upload_part` (repeat for each part)
3. **Monitor**: `list_parts` or `list_multipart_uploads`
4. **Finish**: `complete_multipart_upload`
5. **Cancel (if needed)**: `abort_multipart_upload`
Here’s a **high-level process** for handling **failures in S3 Multipart Uploads** and how to **safely retry**:

### **Multipart Upload with Failure Handling & Retry**

#### **1. Initiate Upload**
- Start with `create_multipart_upload`
- Save the `UploadId` securely (e.g., in a file or database)

#### **2. Upload Parts with Retry Logic**
- Split the file into 5 MB chunks
- For each part:
  - Try uploading using `upload_part`
  - If it fails:
    - Retry up to 3 times with exponential backoff
    - Log errors and continue

#### **3. Handle Crashes or Interruptions**
If the program crashes:
- Use the saved `UploadId`
- Call `list_parts` to see which parts were successfully uploaded
- Resume uploading only the missing parts

#### **4. Complete Upload**
Once all parts are uploaded:
- Call `complete_multipart_upload` with the list of `ETag`s and `PartNumber`s

#### **5. Abort if Needed**
If the upload cannot be completed:
- Call `abort_multipart_upload` to clean up and avoid storage charges

### **Best Practices**
- Always **persist `UploadId` and part metadata**
- Use **`list_multipart_uploads`** to monitor active uploads
- Implement **logging and alerting** for failures
- Use **parallel uploads** for performance, but handle retries independently
```python
import os
import boto3
import logging
import time

# Configure logging
logging.basicConfig(level=logging.INFO)

# AWS credentials from environment variables
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
aws_session_token = os.getenv('AWS_SESSION_TOKEN')  # Optional

# Validate credentials
if not aws_access_key_id or not aws_secret_access_key:
    raise EnvironmentError("AWS credentials not found in environment variables.")

# Create S3 client using environment credentials
s3 = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    aws_session_token=aws_session_token
)

# Configuration
bucket_name = 'your-bucket-name'
file_path = 'your-large-file.zip'
key_name = 'uploads/your-large-file.zip'
chunk_size = 5 * 1024 * 1024  # 5 MB
max_retries = 3

# Step 1: Initiate multipart upload
try:
    response = s3.create_multipart_upload(Bucket=bucket_name, Key=key_name)
    upload_id = response['UploadId']
    logging.info(f"Initiated multipart upload with ID: {upload_id}")
except Exception as e:
    logging.error(f"Failed to initiate multipart upload: {e}")
    raise

# Step 2: Upload parts with retry logic
parts = []
part_number = 1

try:
    with open(file_path, 'rb') as f:
        while True:
            data = f.read(chunk_size)
            if not data:
                break

            for attempt in range(max_retries):
                try:
                    part = s3.upload_part(
                        Bucket=bucket_name,
                        Key=key_name,
                        PartNumber=part_number,
                        UploadId=upload_id,
                        Body=data
                    )
                    parts.append({'ETag': part['ETag'], 'PartNumber': part_number})
                    logging.info(f"Uploaded part {part_number}")
                    break
                except Exception as e:
                    logging.warning(f"Error uploading part {part_number}, attempt {attempt + 1}: {e}")
                    time.sleep(2 ** attempt)
            else:
                raise Exception(f"Failed to upload part {part_number} after {max_retries} attempts.")

            part_number += 1

    # Step 3: Complete multipart upload
    s3.complete_multipart_upload(
        Bucket=bucket_name,
        Key=key_name,
        UploadId=upload_id,
        MultipartUpload={'Parts': parts}
    )
    logging.info("Multipart upload completed successfully.")

except Exception as e:
    logging.error(f"Multipart upload failed: {e}")
    # Abort the upload to avoid orphaned parts
    try:
        s3.abort_multipart_upload(Bucket=bucket_name, Key=key_name, UploadId=upload_id)
        logging.info("Multipart upload aborted.")
    except Exception as abort_error:
        logging.error(f"Failed to abort multipart upload: {abort_error}")
```
## 16. **Transfer Acceleration**
## 17. **Event Notifications**
## 18. **Requester Pays Buckets**

# **Integration and Tools**

## 19. **S3 with AWS Lambda**
## 20. **S3 with AWS Glue and Athena**
## 21. **S3 with CloudFront**
## 22. **S3 with Data Lake Architecture**

# **Monitoring and Logging**

## 23. **S3 Access Logs**
## 24. **CloudTrail Integration**
## 25. **CloudWatch Metrics for S3**

# **Advanced Features**

## 26. **S3 Select**
## 27. **S3 Object Lambda**
## 28. **S3 Inventory**
## 29. **S3 Batch Operations**
